{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEplznWPDdbk2C/1BCMd6l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","# Define the MDP\n","states = ['A', 'B', 'C', 'D']\n","actions = ['left', 'right']\n","\n","# Transition probabilities P(s'|s, a)\n","P = {\n","    'A': {'left': {'A': 1.0}, 'right': {'B': 1.0}},\n","    'B': {'left': {'A': 1.0}, 'right': {'C': 1.0}},\n","    'C': {'left': {'B': 1.0}, 'right': {'D': 1.0}},\n","    'D': {'left': {'C': 1.0}, 'right': {'D': 1.0}}\n","}\n","\n","# Rewards R(s, a)\n","R = {\n","    'A': {'left': 0, 'right': 0},\n","    'B': {'left': 0, 'right': 0},\n","    'C': {'left': 0, 'right': 1},  # reward for moving right from C to D\n","    'D': {'left': 0, 'right': 0}\n","}\n","\n","gamma = 0.9  # discount factor\n","theta = 1e-4  # convergence threshold\n","\n","\n","def policy_evaluation(policy, V):\n","    \"\"\"Evaluate a given policy\"\"\"\n","    while True:\n","        delta = 0\n","        for s in states:\n","            v = V[s]\n","            a = policy[s]\n","            V[s] = sum(P[s][a][s_next] * (R[s][a] + gamma * V[s_next]) for s_next in P[s][a])\n","            delta = max(delta, abs(v - V[s]))\n","        if delta < theta:\n","            break\n","    return V\n","\n","\n","def policy_improvement(V, policy):\n","    \"\"\"Improve policy based on value function\"\"\"\n","    policy_stable = True\n","    for s in states:\n","        old_action = policy[s]\n","        action_values = {}\n","        for a in actions:\n","            action_values[a] = sum(P[s][a][s_next] * (R[s][a] + gamma * V[s_next]) for s_next in P[s][a])\n","        best_action = max(action_values, key=action_values.get)\n","        policy[s] = best_action\n","        if old_action != best_action:\n","            policy_stable = False\n","    return policy, policy_stable\n","\n","\n","def policy_iteration():\n","    \"\"\"Perform policy iteration (evaluation + improvement)\"\"\"\n","    # Initialize policy and value function locally here ✅\n","    policy = {s: np.random.choice(actions) for s in states}\n","    V = {s: 0 for s in states}\n","\n","    iteration = 0\n","    while True:\n","        iteration += 1\n","        print(f\"\\nIteration {iteration}: Policy Evaluation & Improvement\")\n","        V = policy_evaluation(policy, V)\n","        policy, stable = policy_improvement(V, policy)\n","        print(\"Values:\", {s: round(V[s], 3) for s in states})\n","        print(\"Policy:\", policy)\n","        if stable:\n","            print(\"\\n✅ Optimal policy found!\")\n","            break\n","    return policy, V\n","\n","\n","# Run policy iteration\n","optimal_policy, optimal_values = policy_iteration()\n","\n","print(\"\\nFinal Optimal Values and Policy:\")\n","for s in states:\n","    print(f\"State {s}: V = {optimal_values[s]:.4f}, π*(s) = {optimal_policy[s]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIpK4I3k0om4","executionInfo":{"status":"ok","timestamp":1761990640568,"user_tz":-330,"elapsed":59,"user":{"displayName":"LANDA SRINIJA 2448526","userId":"06196680923171560971"}},"outputId":"b26325ac-70ec-46da-9c50-4e599927a545"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Iteration 1: Policy Evaluation & Improvement\n","Values: {'A': 0.81, 'B': 0.9, 'C': 1.0, 'D': 0.0}\n","Policy: {'A': 'right', 'B': 'right', 'C': 'right', 'D': 'left'}\n","\n","Iteration 2: Policy Evaluation & Improvement\n","Values: {'A': 4.263, 'B': 4.736, 'C': 5.263, 'D': 4.737}\n","Policy: {'A': 'right', 'B': 'right', 'C': 'right', 'D': 'left'}\n","\n","✅ Optimal policy found!\n","\n","Final Optimal Values and Policy:\n","State A: V = 4.2628, π*(s) = right\n","State B: V = 4.7365, π*(s) = right\n","State C: V = 5.2628, π*(s) = right\n","State D: V = 4.7365, π*(s) = left\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"f4YSO1hI04Im"},"execution_count":null,"outputs":[]}]}